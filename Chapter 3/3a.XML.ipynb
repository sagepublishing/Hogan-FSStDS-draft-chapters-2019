{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML\n",
    "\n",
    "XML stands for 'extensible mark-up language'. XML files can be generic or have a document type. For exmaple, GraphML is really just XML with a specific schema that is used for social network graph types. \n",
    "\n",
    "Like HTML, XML is a markup language that uses less than ```<``` and greater than ```>``` to encase the element tags. The text inside these tags must have some special characters escaped. \n",
    "\n",
    "~~~ xml \n",
    "<start> \n",
    "    <middle>\n",
    "        <end1>   Here is an element! </end1>\n",
    "        <end2>   Here is an element! </end2>\n",
    "    </middle>\n",
    "</start>\n",
    "~~~\n",
    "\n",
    "Elements have an \"element tree\". Above, ```start``` is the root node, ```middle``` is a child and ```end1``` is a child of middle. ```end1``` and ```end2``` are siblings. \n",
    "\n",
    "XML is a self-documenting style, which means that you can insert details about the elements into the document itself. For example, open up the included Canada.xml file in a text editor.\n",
    "\n",
    "~~~ xml \n",
    "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\n",
    "  <siteinfo>\n",
    "    <sitename>Wikipedia</sitename>\n",
    "    <dbname>enwiki</dbname>\n",
    "\n",
    "~~~ \n",
    "\n",
    "Here you can see the link to the schema. This is a file with a whole ton of details I've never really needed in research. These details are, however, important for specifying what is standard for that type of XML (in this case the mediaWiki export). That's good because it means that well formatted XML should be reliable for it's type and easy to manage. \n",
    "\n",
    "Most of the time, we will not be so concerned with the top of an XML document. Rather, we will simply want to navigate the element tree to get to the element(s) that are of concern to us. Sometimes, parsers will already be written which takes the XML and loads it into a data structure for us. This is the case with graphML, the common format for social network data. To load graphML files you can use either the networkx or igraph packages. I mention these more in Chapter XX on networks. Newer Excel files (with the extension ending in x, as in .xlsx files) are also XML. Later in this chapter we will use pandas to parse those directly. \n",
    "\n",
    "In fact, there is a nice Python library aptly called ```wikipedia``` that can make navigating the XML structure easy and allow for direct querying of all kinds of elements. We are not using that library here, however, since we are making use of Wikipedia but this is to illustrate navigating XML.  \n",
    "\n",
    "In the script below, we will use load in XML as a string. Then we will use beautiful soup to navigate the document and return aspects of the XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<id>864119742</id>\n"
     ]
    }
   ],
   "source": [
    "# loading some xml\n",
    "import bs4, os\n",
    "\n",
    "infile = open(\"..{0}Data{0}Canada.xml\".format(os.sep),'r')\n",
    "\n",
    "wikitext = infile.read()\n",
    "\n",
    "# Note: In some circumstances, the file is saved as encoded data, in which case\n",
    "# use the .decode('utf-8') function on the text. As in:\n",
    "# soup = bs4.BeautifulSoup(wikitext.decode('utf8'), \"lxml\")\n",
    "soup = bs4.BeautifulSoup(wikitext, \"lxml\")\n",
    "\n",
    "print (soup.mediawiki.page.revision.id )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating XML\n",
    "\n",
    "Navigating XML involves moving up and down or sideways along the element tree. In the case above it was clear that I know where to go for the text I wanted (```mediawiki.page.revision.id```). In general, however, navigating to the right element is a bit tedious. Some people prefer the use of Python's built-in ElementTree package. In either case, what you will be doing with your code is navigating a tree structure. Trees tend to use the following nomenclature that borrows from both the natural tree but also the notion of a family tree: \n",
    "- **Root**:The base or primary node is called the root node. \n",
    "- **Parent and child**: A parent is a node that has nodes nested within, like ```ID``` nested within ```revision``` above. In that case, revision is the parent node and ID is the child node. \n",
    "- **Sibling**: Two child nodes with the same parent. Like how ```sitename``` and ```dbname``` are both children of ```siteinfo```.\n",
    "- **Leaf**: A sometiems used term to indicate a child node with no children of it's own. \n",
    "\n",
    "Below I use beautifulsoup to navigate through the tags so that I can get to the data I want. Normally one would do this and then clean it up so that only the proper working code remains. Notice that even though mediawiki is actually at: \n",
    "~~~ html\n",
    "<html>\n",
    "    <body>\n",
    "        <mediawiki>\n",
    "            ...\n",
    "~~~ \n",
    "\n",
    "We do not need the full path to access it, similar to how it was done with HTML. BeautifulSoup will return ```mediawiki``` by going to ```soup.mediawiki.text```. But also note, that this is not the text on the Canada page. Instead it is the text under that leaf node, mediawiki. To get the text of the page from this schema, we would go to ```soup.mediawiki.page``` and get the text from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "Canada\n",
      "0\n",
      "5042916\n",
      "\n",
      "864119742\n",
      "864118763\n",
      "2018-10-15T06:33:55Z\n",
      "\n",
      "Moxy\n",
      "8729451\n",
      "\n",
      "/* Government and politic\n"
     ]
    }
   ],
   "source": [
    "# for i in soup.children: print(i.name)\n",
    "\n",
    "# for i in soup.html.children: print(i.name)\n",
    "\n",
    "# for i in soup.html.body.children: print(i.name)\n",
    "\n",
    "# for i in soup.mediawiki.children: print(i.name) \n",
    "\n",
    "# for i in soup.mediawiki.page.children: print(i.name)\n",
    "\n",
    "# I discover that we can just say soup.page and it will get the text. \n",
    "y = soup.page.text\n",
    "\n",
    "print (soup.page.text == soup.html.body.mediawiki.page.text)\n",
    "print(y[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment there is not much to do with this text. We can probably split it up or count the number of characters. Perhaps you could compare the length of text for Canada to other countries. In the exercise I show how to download this data directly from the special export page. But counting characters will only get us so far in answering questions. In the next chapter we will start parsing this text and adding it to DataFrames. Then in later chapters we will look at including even more data in our DataFrames by comparing data from different topics, sources, accounts, or time periods. First, however, we should look at a couple more data structures. The next one, CSV, being one of the most common formats around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
