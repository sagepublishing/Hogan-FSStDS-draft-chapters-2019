{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "File formats might not be the most exciting topic and certainly one that is often considered far away from traditional social science, or so one might think. In practice, I certainly remember in graduate school the trials of getting data for SPSS and having to convert it to Stata or SAS. Prior to the massive rise in the use of Python and R, quantitative data in social science was almost exclusively done using programs that were 1. for pay, 2. unorthodox syntactically, 3. incompatible. What can be seen from this chapter is actually that Python is for free, that the file formats are not software specific, and that it is assumed that Python should be able to open the data. This is great news not just for data science, but for science. In general, we want science to be as open as possible. Obviously, some data must be restricted for reasons of privacy, but the norm now is towards being less locked into a product. Using Python (or R, for that matter) involves working on a product that is community-driven, open source, freely accessible, and meant to abide by many established conventions in computer science/programming. As we saw at the very end when looking at pickles, when we have to return to a language-specific file format we are right back where we started with version incompatibilities. The pickle thus is not really meant to be a long term storage format, whereas XML, CSV, or JSON can get that job done.\n",
    "\n",
    "The goal of this chapter was to get you acquainted with the logic of many of these tools. The exercises for this chapter will go into a little more effort as we ask just a little more of you than was on offer in the main text. But beyond these exercises, when you start to explore data yourself, you will definitely encounter all manner of contingencies outside of what was mentioned in here. Corrupted files, strange delimiters, and files that are too large to load are all a part of dealing with data science. In a perfect world, all data would be cleaned up, well formatted, and pre-processed. \n",
    "\n",
    "Social science often dreams of that perfect world in a potentially dangerous way. An emphasis on survey research and qualitative coding of transcripts really suggest that claims are made with data of a specific shape and size, rather than data more broadly. We might be inclined to call this the 'independent case model'. It has each row as a case and columns to represent variables. It looks a lot like a DataFrame, and for good reason. The tabulation of cases allows for all kinds of statistical routines that otherwise are not as accessible or tractable. What I am suggesting here, however, is that the process of transforming social life into this table does not have to happen as a part of data collection. The data can be collected from a variety of sources, in a variery of ways. Granted, this can affect generalisability. Independent random sample data collection is still an excellent way to make a generalisable claim. It is instead, to suggest that the drive to make claims generalisable in the mindset of random samples from a large population can sometimes unduly restrict our ability to make claims about a specific population or group. Having all of the comments from a message board or all of the pictures posted in a forum means we can make very extensive claims about that board. Stated differently, we start with the data in the shape it comes in and then work on transforming it into a DataFrame. We do not start by looking for places where life imitates the DataFrame and try to come up with questions to ask.  \n",
    "\n",
    "As we step outside of what we can do with survey research we will find that there's all kinds of ways of creating and managing data for the purposes of making claims. To reiterate some of the things said in the introduction, we are learning to build socialscopes here, not just take whatever scope was presented at the outset.  In the next section we begin to embark on this process of reshaping data so that it can meet our needs. We will have to extract features from text, merge different data sets together, come up with ways to aggregate data (for example, to get averages per day, or within state). This exploratory process will help us learn a little about the shape of the data, what we might be able to expect from it and what sorts of claims we want to make. Data exploration is a journey, and with these tools for getting data into DataFrames, we just got our first vehicle. Let's see where we can go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
